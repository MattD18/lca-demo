{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "1.19.4\n",
      "4.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST\n",
    "\n",
    "Source: https://www.tensorflow.org/tutorials/quickstart/beginner\n",
    "\n",
    "Source: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# training_size\n",
    "training_size = x_train.shape[0]\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Model\n",
    "\n",
    "Source: S7 Additional details on model architectures and training hyperparameters\n",
    "\n",
    "Deciations: 'glorot_uniform' initialization instead of 'He-normal' initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(100, activation=\"relu\", kernel_initializer='glorot_uniform')(inputs)\n",
    "x2 = layers.Dense(50, activation=\"relu\", kernel_initializer='glorot_uniform')(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\", kernel_initializer='glorot_uniform')(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "num_params = (784*100 + 100 + 100*50 + 50 + 50*10 + 10)\n",
    "# loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# Doesn't seem to work\n",
    "# optimizer = tf.keras.optimizers.SGD(\n",
    "#     learning_rate=0.01, momentum=.01,\n",
    "#     name='SGD'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training + Collect LCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tracking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "training_loss_metric = tf.keras.metrics.Mean()\n",
    "training_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "step_loss_list = []\n",
    "\n",
    "# Store weights as rows in # iterations x # parameter matrix\n",
    "step_parameters_over_time = []\n",
    "\n",
    "# store grads as rows in # iterations x # parameter matrix\n",
    "step_grads_over_time = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n"
     ]
    }
   ],
   "source": [
    "# num epochs, paper says they train for 4 epochs and each epoch is 220 steps\n",
    "epochs = 4\n",
    "# batch_size\n",
    "batch_size = training_size // 220\n",
    "# batch_size = training_size // 880\n",
    "\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop + Store LCA Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 161.1017\n",
      "Seen so far: 227 samples\n",
      "Training loss (for one batch) at step 20: 7.0620\n",
      "Seen so far: 4767 samples\n",
      "Training loss (for one batch) at step 40: 2.8598\n",
      "Seen so far: 9307 samples\n",
      "Training loss (for one batch) at step 60: 2.7461\n",
      "Seen so far: 13847 samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=training_size).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "training_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # Store weights at start of step\n",
    "        step_weights = np.array([])\n",
    "        for layer in model.weights:\n",
    "            layer_weights = np.copy(layer.numpy())\n",
    "            # flatten weights \n",
    "            layer_weights = layer_weights.flatten()\n",
    "            # add to iteration weight\n",
    "            step_weights = np.concatenate([step_weights, layer_weights], axis=0)\n",
    "        assert step_weights.shape[0] == num_params\n",
    "        step_parameters_over_time.append(step_weights)\n",
    "        \n",
    "        ## TAKE GRADIENT STEP BASED ON MINIBATCH\n",
    "        \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        # Update loss metric.\n",
    "        training_loss_metric.update_state(loss_value)\n",
    "        \n",
    "        # Log every 20 batches.\n",
    "        if step % 20 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "        \n",
    "        ## RECORD GRADIENT ON FULL TRAINING SET FOR LCA\n",
    "        with tf.GradientTape() as full_tape:\n",
    "            # Run the forward pass of the layer on the entire training set\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            full_logits = model(x_train, training=False)\n",
    "            # Compute the loss value on the entire training set.\n",
    "            full_loss_value = loss_fn(y_train, full_logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        full_grads = full_tape.gradient(full_loss_value, model.trainable_weights)\n",
    "        \n",
    "        # save full training loss\n",
    "        step_loss_list.append(np.copy(full_loss_value.numpy()))\n",
    "        # save full training loss gradient\n",
    "        step_grads = np.array([])\n",
    "        # assumes grads are being concatenated in same order as weights,\n",
    "        # otherwise LCA will be wrongly allocated\n",
    "        for layer in full_grads:\n",
    "            layer_grads = np.copy(layer)\n",
    "            # flatten weights \n",
    "            layer_grads = layer_grads.flatten()\n",
    "            # add to iteration weight\n",
    "            step_grads = np.concatenate([step_grads, layer_grads], axis=0)\n",
    "        assert step_grads.shape[0] == num_params\n",
    "        step_grads_over_time.append(step_grads)\n",
    "    \n",
    "    # Display metrics at the end of each epoch and add to list\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    train_acc_list.append(train_acc)\n",
    "    # save training loss to list\n",
    "    training_loss_result = training_loss_metric.result()\n",
    "    training_loss_list.append(training_loss_result)\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "    training_loss_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "training_end = time.time()\n",
    "\n",
    "print(training_end - training_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at each training iteration, save all parameters as a 1D vector theta_t for iteration t\n",
    "# also save the loss at each iteration L(theta_t)\n",
    "# next for each (iteration -1), take the difference: L(theta_t1) - L(theta_t0)\n",
    "# we can approximate this difference with the dot product via Taylor approximation\n",
    "# i.e. L(theta_t1) - L(theta_t0) = << gradient of loss w.r.t theta, theta_t1 - theta_t0 >>\n",
    "\n",
    "#next decompose dot product into its summands i.e. at time t, \n",
    "# sigma_i ([gradient of loss w.r.t theta] i) * ([theta_t1]_i - [theta_t0]_i)\n",
    "# call each : ([gradient of loss w.r.t theta] i) * ([theta_t1]_i - [theta_t0]_i) the LCA of i at t\n",
    "\n",
    "num_steps = len(step_parameters_over_time)\n",
    "step_lcas_over_time = []\n",
    "for i in range(num_steps - 1):\n",
    "    theta_t0 = step_parameters_over_time[i]\n",
    "    theta_t1 = step_parameters_over_time[i+1]\n",
    "    delta_theta = theta_t1 - theta_t0\n",
    "    grad_theta = step_grads_over_time[i]\n",
    "    summands = grad_theta * delta_theta\n",
    "    step_lcas_over_time.append(summands)\n",
    "    \n",
    "print(len(step_lcas_over_time))\n",
    "step_lcas_over_time = np.stack(step_lcas_over_time)\n",
    "print(step_lcas_over_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/experiment_1/lcas_run_1.pickle', 'wb') as f:\n",
    "    pickle.dump(step_lcas_over_time, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read LCAs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save LCAs\n",
    "with open('../data/experiment_1/lcas_run_1.pickle', 'rb') as f:\n",
    "    step_lcas_over_time = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Train/Val  Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list,label='Train')\n",
    "plt.plot(val_acc_list,label='Val')\n",
    "plt.legend()\n",
    "plt.title(\"Train and Validation Accuracy Per Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step_loss_list)\n",
    "plt.title(\"Training Loss Per Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot delta in training loss compared to Taylor approximation of delta on the step losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_step = 50\n",
    "training_loss_delta = (np.array(step_loss_list)[1:] - np.array(step_loss_list)[:-1])[skip_step:]\n",
    "training_loss_delta_approximation = step_lcas_over_time.sum(1)[skip_step:]\n",
    "plt.plot(training_loss_delta,label='Actual Delta')\n",
    "plt.plot(training_loss_delta_approximation,label='Approx Delta')\n",
    "plt.legend()\n",
    "plt.title(\"Actual and Approximated Training Loss Delta Per Step (skipping first 50 steps)\")\n",
    "plt.ylabel(\"Loss Delta\")\n",
    "plt.xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximation is highly correlated\n",
    "np.corrcoef(training_loss_delta, training_loss_delta_approximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize LCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_lca(lca, tol=.0, zero_cat=1):\n",
    "    category = 0\n",
    "    if lca == 0:\n",
    "        category = zero_cat\n",
    "    elif lca > 0:\n",
    "        category = 2\n",
    "    return category\n",
    "\n",
    "pd.Series(step_lcas_over_time.flatten()).apply(lambda x :categorize_lca(x)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".323 / (.272 + .323), .323 / (.264 + .323), .328 / (.278 + .328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.5428571428571429, 0.5502555366269166, 0.5412541254125411])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std([0.5428571428571429, 0.5502555366269166, 0.5412541254125411]) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.864, 0.744, 0.807]),np.std([0.864, 0.744, 0.807]) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = np.quantile(step_lcas_over_time.flatten(),.01), np.quantile(step_lcas_over_time.flatten(),.99)\n",
    "hist_range = [-1e-5, 1e-5]\n",
    "plt.hist(step_lcas_over_time.flatten()[step_lcas_over_time.flatten() <0], bins=50, range=hist_range,color='green')\n",
    "plt.hist(step_lcas_over_time.flatten()[step_lcas_over_time.flatten() >0], bins=50, range=hist_range,color='red')\n",
    "\n",
    "plt.title('MNIST-FC, Adam: Distribution of all non-zero LCA values',y=1.04)\n",
    "plt.xlabel(f'LCA (censored from {hist_range})')\n",
    "plt.ylabel(f'# of LCAs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = step_lcas_over_time.shape[0]\n",
    "num_weights = step_lcas_over_time.shape[1]\n",
    "pct_positive_lcas_per_step = np.apply_along_axis(lambda x : (x>0).sum(), 1, step_lcas_over_time) / num_weights  \n",
    "pct_zero_lcas_per_step = np.apply_along_axis(lambda x : (x==0).sum(), 1, step_lcas_over_time) / num_weights \n",
    "pct_negative_lcas_per_step = np.apply_along_axis(lambda x : (x<0).sum(), 1, step_lcas_over_time) / num_weights\n",
    "ind = np.arange(num_steps)    # the x locations for the groups\n",
    "width = 1\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,8))\n",
    "\n",
    "p1 = ax1.bar(ind, pct_negative_lcas_per_step, width, bottom=0, color='green', label='helped')\n",
    "p2 = ax1.bar(ind, pct_zero_lcas_per_step, width, bottom=pct_negative_lcas_per_step, color='white', label='zero')\n",
    "p3 = ax1.bar(ind, pct_positive_lcas_per_step, width, bottom=pct_negative_lcas_per_step+pct_zero_lcas_per_step, color='red', label='hurt')\n",
    "ax1.axhline(y=.5, xmin=0, xmax=num_steps, c='black',lw=.4,linestyle='--')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "p0 = ax2.plot(step_loss_list, c='black',lw=2, label='loss')\n",
    "\n",
    "plt.ylabel('percentage')\n",
    "plt.title('MNIST-FC, Adam: Percent helped per iteration')\n",
    "fig.legend(loc=(.7,.75))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz LCA over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "epoch_choice = widgets.IntSlider(value=0, min=0, max=len(step_lcas_over_time)-1)\n",
    "layer_choice = widgets.Select(\n",
    "    options=[0, 2, 4],\n",
    "    value=0,\n",
    ")\n",
    "\n",
    "container = widgets.HBox(children=[epoch_choice, layer_choice])\n",
    "\n",
    "\n",
    "\n",
    "layer_breaks = np.cumsum([78400,100,5000,50, 500,10])\n",
    "\n",
    "layer_index_shape = dict(zip(\n",
    "    layer_breaks,\n",
    "    [layer.numpy().shape for layer in grads]\n",
    "))\n",
    "\n",
    "t = 0\n",
    "\n",
    "def get_formatted_lcas_t(t, viz_layer=0):\n",
    "    formatted_lcas_t = []\n",
    "    lcas_t = step_lcas_over_time[t]\n",
    "    layer_index_start = 0\n",
    "    for layer_index in layer_breaks:\n",
    "        layer_index_end = layer_index\n",
    "        layer_lcas = lcas_t[layer_index_start:layer_index_end]\n",
    "        layer_lcas = layer_lcas.reshape(layer_index_shape[layer_index])\n",
    "        formatted_lcas_t.append(layer_lcas)\n",
    "        layer_index_start = layer_index_end\n",
    "    return formatted_lcas_t[viz_layer]\n",
    "\n",
    "lcas_t_viz_layer = get_formatted_lcas_t(t)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=lcas_t_viz_layer,\n",
    "                    zmid=0,\n",
    "#                     zmax=.001, zmin=-.001,\n",
    "                    colorscale=['green','white','red'])\n",
    "               )\n",
    "interactive_figure = go.FigureWidget(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(change):\n",
    "    lcas_t_viz_layer = get_formatted_lcas_t(epoch_choice.value, viz_layer=layer_choice.value)\n",
    "    with interactive_figure.batch_update():\n",
    "        interactive_figure.data[0].z = lcas_t_viz_layer\n",
    "        \n",
    "epoch_choice.observe(response, names=\"value\")\n",
    "layer_choice.observe(response, names=\"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for 1 seem to be highest at begining, compare LCA vs class accuracy over time\n",
    "\n",
    "widgets.VBox([container,\n",
    "              interactive_figure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
